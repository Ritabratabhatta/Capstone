{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates the data pipeline from raw tables to analytical datasets. At the end of this activity, train & test data sets are created from raw data.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "\n",
    "# standard third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.use_inf_as_na = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard code-template imports\n",
    "from ta_lib.core.api import (\n",
    "    create_context, get_dataframe, get_feature_names_from_column_transformer, get_package_path,\n",
    "    display_as_tabs, string_cleaning, merge_info, initialize_environment,\n",
    "    list_datasets, load_dataset, save_dataset\n",
    ")\n",
    "import ta_lib.eda.api as eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"The default value of regex will change from True to False in a future version.\", \n",
    "                        category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_environment(debug=False, hide_warnings=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = op.join('conf', 'config.yml')\n",
    "context = create_context(config_path)\n",
    "pprint(list_datasets(context))\n",
    "\n",
    "housing_df = load_dataset(context, 'raw/housing')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning and consolidation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>NOTES</u>**\n",
    "\n",
    "The focus here is to create a cleaned dataset that is appropriate for solving the DS problem at hand from the raw data.\n",
    "\n",
    "**1. Do**\n",
    "* clean dataframe column names\n",
    "* ensure dtypes are set properly\n",
    "* join with other tables etc to create features\n",
    "* transform, if appropriate, datetime like columns to generate additional features (weekday etc)\n",
    "* transform, if appropriate, string columns to generate additional features\n",
    "* discard cols that are not useful for training the model (IDs, constant cols, duplicate cols etc)\n",
    "* additional features generated from existing columns\n",
    "\n",
    "\n",
    "**2. Don't**\n",
    "* handle missing values or outliers here. mark them and leave them for processing downstream.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Clean individual tables "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products Table\n",
    "\n",
    "From data discovery, we know the following\n",
    "\n",
    "* all columns are strings : nothing to fix. Apply generic cleaning (strip extra whitespace etc)\n",
    "* ensure all `invalid` string entries are mapped to np.NaN\n",
    "* some column are duplicates (eg. color, Ext_Color). Better to `coalesce` them instead of an outright discard of one of the columns.\n",
    "* SKU is key column : ensure no duplicate values\n",
    "* This will go into production code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_clean = (\n",
    "    housing_df\n",
    "\n",
    "    .copy()\n",
    "\n",
    "    # set dtypes : nothing to do here\n",
    "    .passthrough()\n",
    "    \n",
    "    .replace({'': np.NaN})\n",
    "    \n",
    "    .clean_names(case_type='snake')\n",
    "\n",
    ")\n",
    "housing_df_clean.dropna(subset=['total_bedrooms'], inplace=True)\n",
    "\n",
    "housing_df_clean.reset_index(drop=True,inplace = True)\n",
    "housing_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_clean.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "It's always a good idea to save cleaned tabular data using a storage format that supports the following \n",
    "\n",
    "1. preserves the type information\n",
    "2. language agnostic storage format\n",
    "3. Supports compression\n",
    "4. Supports customizing storage to optimize different data access patterns\n",
    "\n",
    "For larger datasets, the last two points become crucial.\n",
    "\n",
    "`Parquet` is one such file format that is very popular for storing tabular data. It has some nice properties:\n",
    "- Similar to pickles & RDS datasets, but compatible with all languages\n",
    "- Preserves the datatypes\n",
    "- Compresses the data and reduces the filesize\n",
    "- Good library support in Python and other languages\n",
    "- As a columnar storage we can efficiently read fewer columns\n",
    "- It also supports chunking data by groups of columns (for instance, by dates or a particular value of a key column) that makes loading subsets of the data fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, housing_df_clean, 'cleaned/housing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Train, Validation and Test datasets\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We split the data into train, test (optionally, also a validation dataset)\n",
    "- In this example, we are binning the target into 10 quantiles and then use a Stratified Shuffle to split the data.\n",
    "- See sklearn documentation on the various available splitters\n",
    "- https://scikit-learn.org/stable/modules/classes.html#splitter-classes\n",
    "- This will go into production code (training only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from ta_lib.core.api import custom_train_test_split  # helper function to customize splitting\n",
    "from scripts import *\n",
    "\n",
    "\n",
    "housing_df_clean[\"income_cat\"] = pd.cut(\n",
    "    housing_df_clean[\"median_income\"],\n",
    "    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5],\n",
    ")\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing_df_clean, housing_df_clean[\"income_cat\"]):\n",
    "    strat_train_set = housing_df_clean.loc[train_index]\n",
    "    strat_test_set = housing_df_clean.loc[test_index]\n",
    "\n",
    "\n",
    "# splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=context.random_seed)\n",
    "# strat_train_set, strat_test_set = custom_train_test_split(housing_df_clean, splitter, by=binned_selling_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "housing_tr_c = strat_train_set.copy()\n",
    "housing_tr_c.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr_c.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr_c.drop(\"ocean_proximity\", axis=1, inplace=True)\n",
    "corr_matrix = housing_tr_c.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr_features = strat_train_set.drop(\n",
    "    \"median_house_value\", axis=1\n",
    ")  # drop labels for training set\n",
    "housing_tr_target = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "housing_tr_f_num = housing_tr_features.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "imputer.fit(housing_tr_f_num)\n",
    "X = imputer.transform(housing_tr_f_num)\n",
    "\n",
    "housing_tr_fe = pd.DataFrame(X, columns=housing_tr_f_num.columns, index=housing_tr_features.index)\n",
    "housing_tr_fe[\"rooms_per_household\"] = housing_tr_fe[\"total_rooms\"] / housing_tr_fe[\"households\"]\n",
    "housing_tr_fe[\"bedrooms_per_room\"] = (\n",
    "    housing_tr_fe[\"total_bedrooms\"] / housing_tr_fe[\"total_rooms\"]\n",
    ")\n",
    "housing_tr_fe[\"population_per_household\"] = (\n",
    "    housing_tr_fe[\"population\"] / housing_tr_fe[\"households\"]\n",
    ")\n",
    "\n",
    "housing_tr_cat = housing_tr_features[[\"ocean_proximity\"]]\n",
    "housing_tr_features = housing_tr_fe.join(pd.get_dummies(housing_tr_cat, drop_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_te_features = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "housing_te_target = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "housing_te_f_num = housing_te_features.drop(\"ocean_proximity\", axis=1)\n",
    "X_test_prepared = imputer.transform(housing_te_f_num)\n",
    "housing_te_fe = pd.DataFrame(\n",
    "    X_test_prepared, columns=housing_te_f_num.columns, index=housing_te_features.index\n",
    ")\n",
    "housing_te_fe[\"rooms_per_household\"] = (\n",
    "    housing_te_fe[\"total_rooms\"] / housing_te_fe[\"households\"]\n",
    ")\n",
    "housing_te_fe[\"bedrooms_per_room\"] = (\n",
    "    housing_te_fe[\"total_bedrooms\"] / housing_te_fe[\"total_rooms\"]\n",
    ")\n",
    "housing_te_fe[\"population_per_household\"] = (\n",
    "    housing_te_fe[\"population\"] / housing_te_fe[\"households\"]\n",
    ")\n",
    "\n",
    "housing_te_cat = housing_te_features[[\"ocean_proximity\"]]\n",
    "housing_te_features = housing_te_fe.join(pd.get_dummies(housing_te_cat, drop_first=True))\n",
    "housing_te_features['ocean_proximity_ISLAND'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_X, train_y = (\n",
    "#     sales_df_train\n",
    "    \n",
    "#     # split the dataset to train and test\n",
    "#     .get_features_targets(target_column_names=target_col)\n",
    "# )\n",
    "save_dataset(context, housing_tr_features, 'train/housing/features')\n",
    "save_dataset(context, housing_tr_target, 'train/housing/target')\n",
    "\n",
    "\n",
    "# test_X, test_y = (\n",
    "#     sales_df_test\n",
    "    \n",
    "#     # split the dataset to train and test\n",
    "#     .get_features_targets(target_column_names=target_col)\n",
    "# )\n",
    "save_dataset(context, housing_te_features, 'test/housing/features')\n",
    "save_dataset(context, housing_te_target, 'test/housing/target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_te_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
